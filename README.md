# Machine-Learning
Author: Timothy Holland \
Last Updated: 05/06/2024

Slowly but surely my machine learning skills have faded. Here is my attempt at fighting off those fuzzies. Currently, I've only completed a couple of Kaggle competitions as a refresher. For some more advanced, and arguably more impressive moments in my "AI career", check out my university portfolio. There I was blessed with the daemon of skeleton codes and deadlines! \
\
Within this repository, there is one non-Kaggle machine learning thing - my attempt at an auto-encoder of the MNIST dataset. I guess that's just a layman's Kaggle in a way. I'm trying to think about how to expand beyond Kaggle, but for the moment, I'll be like everyone else...

### Curiosity

- MNIST Autoencoder
  - From initial murmurings on the variational auto-encoders used in generative AI. I was a tad confused.
  - It does work though! I think there's another program on MNIST clothes too, if that's in fashion.

### Kaggle Competitions

- Titanic
  - Source: https://www.kaggle.com/competitions/titanic/overview
  - Performance: 0.77511 on test dataset, 8180 out of 16000
  - Best model: minor feature editing, random forest with grid search.
  - To be fair to myself, most submissions by other people would surely constitute cheating, seeing as they've just uploaded the publicly available dataset. I'm not being a snitch.
- Housing
  - Source: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview
  - Performance: 0.14719 on test dataset, 2545 out of 4916
  - Best model: a simple pipeline to impute values, with random forest.
  - I'm working on it!
- Bag of Words Meets Bags of Popcorn
  - Source: https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview
  - Performance: 0.86472
  - Best model: logistic regression with self-training on unlabelled data.
  - Tried to incorporate transfer learning, but too computationally expensive.
