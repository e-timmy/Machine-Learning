{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c49cc4b",
   "metadata": {},
   "source": [
    "# Bag of Words Meets Bags of Popcorn\n",
    "\n",
    "Filename: movie-review-model.ipynb \\\n",
    "Author: Timothy Holland \\\n",
    "Last updated: 14/06/2024 \\\n",
    "Kaggle competition: https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ef720",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfa11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('word2vec-nlp-tutorial/labeledTrainData.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1997ea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the data:\n",
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "\n",
      "Total number of samples: 25000\n",
      "\n",
      "Sentiment distribution:\n",
      "1: 12500 (50.00%)\n",
      "0: 12500 (50.00%)\n",
      "\n",
      "Average review length: 1327.71 characters\n",
      "\n",
      "Number of unique words: 257663\n",
      "\n",
      "Random sample:\n",
      "ID: 6827_10\n",
      "Review: I spent 5 hours drenched in this film. Nothing I have ever seen comes close to the delicious funk this film left me in. Never mind females advanced aging dilemma's, human fear vaults off the screen fo...\n",
      "Sentiment: 1\n",
      "\n",
      "Correlation between review length and sentiment: 0.0219\n",
      "\n",
      "Top 10 most common words:\n",
      "the: 322198\n",
      "a: 159949\n",
      "and: 158572\n",
      "of: 144462\n",
      "to: 133967\n",
      "is: 104170\n",
      "in: 90527\n",
      "i: 70480\n",
      "this: 69711\n",
      "that: 66292\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Print the first few rows of the data\n",
    "print(\"First few rows of the data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Get the total number of samples\n",
    "total_samples = len(data)\n",
    "print(f\"\\nTotal number of samples: {total_samples}\")\n",
    "\n",
    "# Get the unique sentiments and their counts\n",
    "sentiment_counts = Counter(data['sentiment'])\n",
    "print(\"\\nSentiment distribution:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count} ({count/total_samples*100:.2f}%)\")\n",
    "\n",
    "# Get the average length of the reviews\n",
    "avg_review_length = data['review'].apply(len).mean()\n",
    "print(f\"\\nAverage review length: {avg_review_length:.2f} characters\")\n",
    "\n",
    "# Get the number of unique words in the reviews\n",
    "unique_words = set()\n",
    "data['review'].str.lower().str.split().apply(unique_words.update)\n",
    "print(f\"\\nNumber of unique words: {len(unique_words)}\")\n",
    "\n",
    "# Print a random sample from the data\n",
    "print(\"\\nRandom sample:\")\n",
    "sample = data.sample().iloc[0]\n",
    "print(f\"ID: {sample['id']}\")\n",
    "print(f\"Review: {sample['review'][:200]}...\") # Print first 200 characters\n",
    "print(f\"Sentiment: {sample['sentiment']}\")\n",
    "\n",
    "# Calculate correlation between review length and sentiment\n",
    "data['review_length'] = data['review'].apply(len)\n",
    "correlation = data['review_length'].corr(data['sentiment'])\n",
    "print(f\"\\nCorrelation between review length and sentiment: {correlation:.4f}\")\n",
    "\n",
    "# Print the most common words\n",
    "word_counts = Counter()\n",
    "data['review'].str.lower().str.split().apply(word_counts.update)\n",
    "print(\"\\nTop 10 most common words:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc4626",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e270e44",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b60fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data:\n",
      "       id  sentiment                                preprocessed_review\n",
      "0  5814_8          1  stuff going moment mj ive started listening mu...\n",
      "1  2381_9          1  classic war world timothy hines entertaining f...\n",
      "2  7759_3          0  film start manager nicholas bell giving welcom...\n",
      "3  3630_4          0  must assumed praised film greatest filmed oper...\n",
      "4  9495_8          1  superbly trashy wondrously unpretentious explo...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data (uncomment these lines if you haven't downloaded them before)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing functions\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocessing function to the 'review' column\n",
    "data['preprocessed_review'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "# Print the first few rows of the preprocessed data\n",
    "print(\"Preprocessed data:\")\n",
    "print(data[['id', 'sentiment', 'preprocessed_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748036d",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c427aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the preprocessed reviews into words\n",
    "tokenized_reviews = [review.split() for review in data['preprocessed_review']]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "embedding_size = 100\n",
    "window_size = 5\n",
    "min_word_count = 5\n",
    "\n",
    "model = Word2Vec(tokenized_reviews, vector_size=embedding_size, window=window_size, min_count=min_word_count)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocabulary = list(model.wv.index_to_key)\n",
    "\n",
    "# Get the word vectors\n",
    "word_vectors = model.wv[vocabulary]\n",
    "\n",
    "# Perform t-SNE dimensionality reduction for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the word embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, word in enumerate(vocabulary):\n",
    "    x, y = embeddings_2d[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
    "\n",
    "plt.title('Word Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n",
    "\n",
    "# Find similar words\n",
    "query_word = 'great'\n",
    "similar_words = model.wv.most_similar(query_word, topn=10)\n",
    "print(f\"Similar words to '{query_word}':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79286f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
