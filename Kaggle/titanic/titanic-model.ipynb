{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034e5cee",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "Filename: titanic-model.ipynb \\\n",
    "Author: Timothy Holland \\\n",
    "Last updated: 17/05/2024 \\\n",
    "Kaggle competition: https://www.kaggle.com/competitions/titanic/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22278cc8",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "#### Uploading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d4e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset into DataFrames\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Split training into features and target variable\n",
    "x_train = train_df.drop(['PassengerId', 'Survived'], axis=1)\n",
    "y_train = train_df['Survived']\n",
    "# Split test into features and target variable\n",
    "x_test = test_df.drop(['PassengerId'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f731247",
   "metadata": {},
   "source": [
    "### 1.1 Feature Engineering\n",
    "### Defining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98fad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "Non-numeric: ['Sex', 'Ticket', 'Cabin', 'Embarked', 'Title']\n"
     ]
    }
   ],
   "source": [
    "# Extract titles from the 'Name' column\n",
    "x_train['Title'] = x_train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "x_test['Title'] = x_test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Drop 'Name' column\n",
    "x_train.drop('Name', axis=1, inplace=True)\n",
    "x_test.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "# Specify numeric and non-numeric columns\n",
    "numeric_cols = x_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "non_numeric_cols = x_test.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric: {numeric_cols}\")\n",
    "print(f\"Non-numeric: {non_numeric_cols}\")\n",
    "# # Missing numerical values filled with average\n",
    "# train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].mean())\n",
    "# test_df[numeric_cols] = test_df[numeric_cols].fillna(test_df[numeric_cols].mean())\n",
    "\n",
    "# # Missing non-numeric values filled with mode\n",
    "# for col in non_numeric_cols:\n",
    "#     train_df[col].fillna(train_df[col].mode()[0], inplace=True)\n",
    "#     test_df[col].fillna(test_df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25305fe",
   "metadata": {},
   "source": [
    "### Analysing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbf01f",
   "metadata": {},
   "source": [
    "#### Transforming Non-numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6114be27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Sex\n",
      "Sex\n",
      "male      577\n",
      "female    314\n",
      "Name: count, dtype: int64\n",
      "Missing Values: 0 (0.00%)\n",
      "Test distribution\n",
      "Sex\n",
      "male      266\n",
      "female    152\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature: Ticket\n",
      "Ticket\n",
      "347082      7\n",
      "CA. 2343    7\n",
      "1601        7\n",
      "3101295     6\n",
      "CA 2144     6\n",
      "           ..\n",
      "9234        1\n",
      "19988       1\n",
      "2693        1\n",
      "PC 17612    1\n",
      "370376      1\n",
      "Name: count, Length: 681, dtype: int64\n",
      "Missing Values: 0 (0.00%)\n",
      "Test distribution\n",
      "Ticket\n",
      "PC 17608    5\n",
      "CA. 2343    4\n",
      "113503      4\n",
      "PC 17483    3\n",
      "220845      3\n",
      "           ..\n",
      "349226      1\n",
      "2621        1\n",
      "4133        1\n",
      "113780      1\n",
      "2668        1\n",
      "Name: count, Length: 363, dtype: int64\n",
      "\n",
      "Feature: Cabin\n",
      "Cabin\n",
      "NaN            687\n",
      "C23 C25 C27      4\n",
      "G6               4\n",
      "B96 B98          4\n",
      "C22 C26          3\n",
      "              ... \n",
      "E34              1\n",
      "C7               1\n",
      "C54              1\n",
      "E36              1\n",
      "C148             1\n",
      "Name: count, Length: 148, dtype: int64\n",
      "Missing Values: 687 (77.10%)\n",
      "Test distribution\n",
      "Cabin\n",
      "NaN                327\n",
      "B57 B59 B63 B66      3\n",
      "C89                  2\n",
      "C116                 2\n",
      "C80                  2\n",
      "                  ... \n",
      "E45                  1\n",
      "E52                  1\n",
      "B58 B60              1\n",
      "C62 C64              1\n",
      "C105                 1\n",
      "Name: count, Length: 77, dtype: int64\n",
      "\n",
      "Feature: Embarked\n",
      "Embarked\n",
      "S      644\n",
      "C      168\n",
      "Q       77\n",
      "NaN      2\n",
      "Name: count, dtype: int64\n",
      "Missing Values: 2 (0.22%)\n",
      "Test distribution\n",
      "Embarked\n",
      "S    270\n",
      "C    102\n",
      "Q     46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature: Title\n",
      "Title\n",
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Mlle          2\n",
      "Major         2\n",
      "Col           2\n",
      "Countess      1\n",
      "Capt          1\n",
      "Ms            1\n",
      "Sir           1\n",
      "Lady          1\n",
      "Mme           1\n",
      "Don           1\n",
      "Jonkheer      1\n",
      "Name: count, dtype: int64\n",
      "Missing Values: 0 (0.00%)\n",
      "Test distribution\n",
      "Title\n",
      "Mr        240\n",
      "Miss       78\n",
      "Mrs        72\n",
      "Master     21\n",
      "Col         2\n",
      "Rev         2\n",
      "Ms          1\n",
      "Dr          1\n",
      "Dona        1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Examine the unique values, their frequencies, and missing values for non-numeric features\n",
    "for feature in non_numeric_cols:\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(x_train[feature].value_counts(dropna=False))\n",
    "    \n",
    "    null_count = x_train[feature].isnull().sum()\n",
    "    null_percentage = null_count / len(x_train) * 100\n",
    "    print(f\"Missing Values: {null_count} ({null_percentage:.2f}%)\")\n",
    "    print(\"Test distribution\")\n",
    "    print(x_test[feature].value_counts(dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6bcfa",
   "metadata": {},
   "source": [
    "##### 'Ticket'\n",
    "Feature is categorical and sparse, applying label encoding over one-hot to reduce dimensionality. Therefore, there is a potential problem for ordering to affect the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ddf2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Convert 'Cabin' values to strings\n",
    "x_train['Ticket'] = x_train['Ticket'].astype(str)\n",
    "x_test['Ticket'] = x_test['Ticket'].astype(str)\n",
    "\n",
    "# Concatenate the training and test sets for fitting the label encoder\n",
    "x_ticket = pd.concat([x_train['Ticket'], x_test['Ticket']])\n",
    "\n",
    "# Fit the label encoder on the combined data\n",
    "le = LabelEncoder()\n",
    "le.fit(x_ticket)\n",
    "\n",
    "x_train['Ticket'] = le.transform(x_train['Ticket'])\n",
    "x_test['Ticket'] = le.transform(x_test['Ticket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524c077",
   "metadata": {},
   "source": [
    "##### 'Cabin'\n",
    "Feature has majority missing values, categorical, and sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe5eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create category for missing values\n",
    "x_train['Cabin'] = x_train['Cabin'].fillna('Unknown')\n",
    "x_test['Cabin'] = x_test['Cabin'].fillna('Unknown')\n",
    "\n",
    "# Convert 'Cabin' values to strings\n",
    "x_train['Cabin'] = x_train['Cabin'].astype(str)\n",
    "x_test['Cabin'] = x_test['Cabin'].astype(str)\n",
    "\n",
    "# Concatenate the training and test sets for fitting the label encoder\n",
    "x_cabin = pd.concat([x_train['Cabin'], x_test['Cabin']])\n",
    "\n",
    "# Fit the label encoder on the combined data\n",
    "le = LabelEncoder()\n",
    "le.fit(x_cabin)\n",
    "\n",
    "# Transform the training and test sets separately\n",
    "x_train['Cabin'] = le.transform(x_train['Cabin'])\n",
    "x_test['Cabin'] = le.transform(x_test['Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90d654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    186\n",
      "1    106\n",
      "2    186\n",
      "3     70\n",
      "4    186\n",
      "Name: Cabin, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display 'Cabin' information\n",
    "print(x_train['Cabin'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d321d4",
   "metadata": {},
   "source": [
    "##### 'Embarked' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff005269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Replace unknowns with mode\n",
    "most_frequent_value = x_train['Embarked'].mode()[0]\n",
    "x_train['Embarked'].fillna(most_frequent_value, inplace=True)\n",
    "x_train['Embarked'].fillna(most_frequent_value, inplace=True)\n",
    "\n",
    "# Reshape the training data to be 2D\n",
    "x_train_embarked = x_train['Embarked'].values.reshape(-1, 1)\n",
    "\n",
    "print(x_train['Embarked'].unique())\n",
    "\n",
    "# Fit the encoder on the training data\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(x_train_embarked)\n",
    "\n",
    "# Transform training and test data\n",
    "x_train_embarked_encoded = ohe.transform(x_train_embarked).toarray()\n",
    "x_test_embarked_encoded = ohe.transform(x_test['Embarked'].values.reshape(-1, 1)).toarray()\n",
    "embarked_encoded_columns = ohe.get_feature_names_out(['Embarked'])\n",
    "\n",
    "#  Drop embarked and replace with OHE\n",
    "x_train = x_train.drop('Embarked', axis=1)\n",
    "x_test = x_test.drop('Embarked', axis=1)\n",
    "x_train = pd.concat([x_train, pd.DataFrame(x_train_embarked_encoded, columns=embarked_encoded_columns)], axis=1)\n",
    "x_test = pd.concat([x_test, pd.DataFrame(x_test_embarked_encoded, columns=embarked_encoded_columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966a260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin',\n",
      "       'Title', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(x_train.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad57032",
   "metadata": {},
   "source": [
    "##### 'Sex'\n",
    "Binary encoding of categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "199c5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding of 'Sex'\n",
    "x_train['Sex'] = x_train['Sex'].replace({'male': 0, 'female': 1})\n",
    "x_test['Sex'] = x_test['Sex'].replace({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1ca26",
   "metadata": {},
   "source": [
    "##### 'Title'\n",
    "One hot encoding of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3fb0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regroup title categories\n",
    "\n",
    "def group_titles(title):\n",
    "    if title in ['Mr', 'Miss', 'Mrs', 'Master']:\n",
    "        return title\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "x_train['Title'] = x_train['Title'].apply(group_titles)\n",
    "x_test['Title'] = x_test['Title'].apply(group_titles)\n",
    "\n",
    "\n",
    "# Apply one-hot-encoding\n",
    "x_train = pd.get_dummies(x_train, columns=['Title'])\n",
    "x_test = pd.get_dummies(x_test, columns=['Title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62533209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin',\n",
      "       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Miss',\n",
      "       'Title_Mr', 'Title_Mrs', 'Title_Other'],\n",
      "      dtype='object')\n",
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin',\n",
      "       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Miss',\n",
      "       'Title_Mr', 'Title_Mrs', 'Title_Other'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(x_train.keys())\n",
    "print(x_test.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae020e",
   "metadata": {},
   "source": [
    "#### Transforming Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6be131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Features:\n",
      "        count       mean        std   min      25%      50%   75%       max\n",
      "Pclass  891.0   2.308642   0.836071  1.00   2.0000   3.0000   3.0    3.0000\n",
      "Age     714.0  29.699118  14.526497  0.42  20.1250  28.0000  38.0   80.0000\n",
      "SibSp   891.0   0.523008   1.102743  0.00   0.0000   0.0000   1.0    8.0000\n",
      "Parch   891.0   0.381594   0.806057  0.00   0.0000   0.0000   0.0    6.0000\n",
      "Fare    891.0  32.204208  49.693429  0.00   7.9104  14.4542  31.0  512.3292\n",
      "\n",
      "Missing Values:\n",
      "Pclass: 0 (0.00%)\n",
      "Age: 177 (19.87%)\n",
      "SibSp: 0 (0.00%)\n",
      "Parch: 0 (0.00%)\n",
      "Fare: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics and missing values for numeric features\n",
    "print(\"Numeric Features:\")\n",
    "print(x_train[numeric_cols].describe().transpose())\n",
    "\n",
    "null_counts = x_train[numeric_cols].isnull().sum()\n",
    "null_percentages = null_counts / len(x_train) * 100\n",
    "print(\"\\nMissing Values:\")\n",
    "for feature, count, percentage in zip(numeric_cols, null_counts, null_percentages):\n",
    "    print(f\"{feature}: {count} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3cd64",
   "metadata": {},
   "source": [
    "##### 'Pclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83120334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6b849",
   "metadata": {},
   "source": [
    "##### 'Age'\n",
    "KNN imputation of missing values (n=178). \\\n",
    "Last feature with missing values.\\\n",
    "Normalisation to [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2fc22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create imputer object\n",
    "imputer = KNNImputer(n_neighbors=5) \n",
    "\n",
    "# Fix imputer on training data\n",
    "x_train_imputed = imputer.fit_transform(x_train)\n",
    "x_test_imputed = imputer.transform(x_test)\n",
    "\n",
    "# Create dataframe\n",
    "x_train = pd.DataFrame(x_train_imputed, columns=x_train.columns, index=x_train.index)\n",
    "x_test = pd.DataFrame(x_test_imputed, columns=x_test.columns, index=x_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ddf121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    891.000000\n",
      "mean      29.471969\n",
      "std       13.618072\n",
      "min        0.420000\n",
      "25%       21.000000\n",
      "50%       28.200000\n",
      "75%       36.700000\n",
      "max       80.000000\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(x_train['Age'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "423b7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create instance of scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalise age column\n",
    "x_train['Age'] = scaler.fit_transform(x_train[['Age']])\n",
    "x_test['Age'] = scaler.transform(x_test[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "934ecaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    891.000000\n",
      "mean       0.365066\n",
      "std        0.171124\n",
      "min        0.000000\n",
      "25%        0.258608\n",
      "50%        0.349083\n",
      "75%        0.455893\n",
      "max        1.000000\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(x_train['Age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350e25a",
   "metadata": {},
   "source": [
    "##### 'Sibsp: # of siblings / spouses aboard the Titanic\n",
    "Normalisation applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e0be566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create instance of scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalise age column\n",
    "x_train['SibSp'] = scaler.fit_transform(x_train[['SibSp']])\n",
    "x_test['SibSp'] = scaler.transform(x_test[['SibSp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0232921",
   "metadata": {},
   "source": [
    "##### 'Parch': # of parents / children aboard the Titanic\n",
    "Normalisation applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdd2c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create instance of scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalise age column\n",
    "x_train['Parch'] = scaler.fit_transform(x_train[['Parch']])\n",
    "x_test['Parch'] = scaler.transform(x_test[['Parch']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ce7f9",
   "metadata": {},
   "source": [
    "##### 'Fare': cost of ticket\n",
    "Normalisation applied.\n",
    "Apply logarithmic transformation (right skewed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fc48444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create instance of scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply logarithmic transformation\n",
    "x_train['Fare'] = np.log1p(x_train['Fare'])\n",
    "x_test['Fare'] = np.log1p(x_test['Fare'])\n",
    "\n",
    "# Normalise age column\n",
    "x_train['Fare'] = scaler.fit_transform(x_train[['Fare']])\n",
    "x_test['Fare'] = scaler.transform(x_test[['Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2b641",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "318dc41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have your features (X) and target variable (y) defined\n",
    "\n",
    "# Split the data into training and evaluation sets (80-20 split)\n",
    "x_train, x_evaluation, y_train, y_evaluation = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ddcf3",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a2827",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a674c86",
   "metadata": {},
   "source": [
    "##### Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8773f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in /opt/anaconda3/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikeras) (3.3.3)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikeras) (1.4.2)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from optree->keras>=3.2.0->scikeras) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->scikeras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->scikeras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22b64eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88165b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'optimizer': 'adam', 'activation': 'tanh', 'neurons': 128, 'batch_size': 32, 'epochs': 10, 'mean_cv_score': 0.8118881225585938}\n",
      "Epoch 1/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - accuracy: 0.5135 - loss: 0.7943\n",
      "Epoch 2/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.6528 - loss: 0.6276\n",
      "Epoch 3/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - accuracy: 0.6536 - loss: 0.6057\n",
      "Epoch 4/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.6934 - loss: 0.5963\n",
      "Epoch 5/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.6521 - loss: 0.6191\n",
      "Epoch 6/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - accuracy: 0.6877 - loss: 0.6026\n",
      "Epoch 7/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - accuracy: 0.6617 - loss: 0.6180\n",
      "Epoch 8/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431us/step - accuracy: 0.6829 - loss: 0.5887\n",
      "Epoch 9/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - accuracy: 0.6728 - loss: 0.6119\n",
      "Epoch 10/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.7045 - loss: 0.5900\n",
      "Test Loss: 0.6354\n",
      "Test Accuracy: 0.6816\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Convert x_train and y_train to NumPy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'neurons': [32, 64, 128],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [10, 20]\n",
    "}\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform grid search\n",
    "for optimizer in param_grid['optimizer']:\n",
    "    for activation in param_grid['activation']:\n",
    "        for neurons in param_grid['neurons']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                for epochs in param_grid['epochs']:\n",
    "                    # Create the model with the current hyperparameters\n",
    "                    model = keras.Sequential([\n",
    "                        keras.layers.Dense(neurons, activation=activation, input_shape=(x_train.shape[1],)),\n",
    "                        keras.layers.Dense(32, activation=activation),\n",
    "                        keras.layers.Dense(1, activation='sigmoid')\n",
    "                    ])\n",
    "                    model.compile(optimizer=optimizer,\n",
    "                                  loss='binary_crossentropy',\n",
    "                                  metrics=['accuracy'])\n",
    "                    \n",
    "                    # Define the number of folds for cross-validation\n",
    "                    n_folds = 5\n",
    "                    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "                    \n",
    "                    # Perform cross-validation\n",
    "                    cv_scores = []\n",
    "                    for train_index, val_index in kfold.split(x_train, y_train):\n",
    "                        x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "                        \n",
    "                        # Train the model\n",
    "                        model.fit(x_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "                        \n",
    "                        # Evaluate the model on the validation set\n",
    "                        scores = model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
    "                        cv_scores.append(scores[1])  # Store the accuracy score\n",
    "                    \n",
    "                    # Calculate the mean cross-validation score\n",
    "                    mean_cv_score = np.mean(cv_scores)\n",
    "                    \n",
    "                    # Store the results\n",
    "                    results.append({\n",
    "                        'optimizer': optimizer,\n",
    "                        'activation': activation,\n",
    "                        'neurons': neurons,\n",
    "                        'batch_size': batch_size,\n",
    "                        'epochs': epochs,\n",
    "                        'mean_cv_score': mean_cv_score\n",
    "                    })\n",
    "\n",
    "# Find the best hyperparameters and score\n",
    "best_result = max(results, key=lambda x: x['mean_cv_score'])\n",
    "print(\"Best hyperparameters: \", best_result)\n",
    "\n",
    "# Train the model with the best hyperparameters on the entire training set\n",
    "best_model = keras.Sequential([\n",
    "    keras.layers.Dense(best_result['neurons'], activation=best_result['activation'], input_shape=(x_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation=best_result['activation']),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "best_model.compile(optimizer=best_result['optimizer'],\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "best_model.fit(x_train, y_train, batch_size=best_result['batch_size'], epochs=best_result['epochs'], verbose=1)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(x_evaluation, y_evaluation, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deebc58",
   "metadata": {},
   "source": [
    "##### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Convert x_train and y_train to NumPy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a StratifiedKFold object\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = []\n",
    "for train_index, val_index in kfold.split(x_train, y_train):\n",
    "    # Split the data into train and validation sets for the current fold\n",
    "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # Create a new model instance\n",
    "    model = create_model()\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    model.fit(x_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Evaluate the model on the validation set for the current fold\n",
    "    scores = model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
    "    cv_scores.append(scores[1])  # Store the accuracy score\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "print(f'Mean cross-validation score: {np.mean(cv_scores):.4f}')\n",
    "\n",
    "# Train the final model on the entire training set\n",
    "final_model = create_model()\n",
    "final_model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_loss, test_accuracy = final_model.evaluate(x_evaluation, y_evaluation, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425415b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
